{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with embeddings: gender + wasBornIn, city isLocatedIn, scientists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV , train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from kge.util.io import load_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load entities and their embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Load entity type Dataframe\n",
    "entities_df = pd.read_csv('data/entities_types.csv', sep=',', header=0)\n",
    "\n",
    "#load YAGO Embeddings\n",
    "checkpoint = load_checkpoint('../embeddings/yago3-10-complex.pt') # Change the model here\n",
    "model = checkpoint[\"model\"]\n",
    "tensor_embed: torch.Tensor = dict(checkpoint['model'][0])['_entity_embedder._embeddings.weight']\n",
    "all_arrays = tensor_embed.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading triples...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset to get the relations\n",
    "print(\"Loading triples...\")\n",
    "GLOBAL_PATH = \"data/\"\n",
    "NAME_SUBGRAPH = [\"train\", \"test\", \"valid\"]\n",
    "cols_name = [\"head\", \"relation\", \"tail\"]\n",
    "triples_df = pd.concat(\n",
    "    (pd.read_csv(\n",
    "        GLOBAL_PATH + f + '.txt',\n",
    "        sep=\"\\t\",\n",
    "        names=cols_name) for f in NAME_SUBGRAPH),\n",
    "    ignore_index=True)\n",
    "# Get distinct relations\n",
    "distinct_relations = triples_df[\"relation\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if the relations exists...\n"
     ]
    }
   ],
   "source": [
    "# Dictionnary for all the boolean value of relation regarding entitiy\n",
    "value_relations = {relation: set() for relation in distinct_relations}\n",
    "\n",
    "print(\"Check if the relations exists...\")\n",
    "for _, row in triples_df.iterrows():\n",
    "    # Extract entity and relations\n",
    "    head = row[\"head\"]\n",
    "    relation = row[\"relation\"]\n",
    "    # Add the entity to the corresponding set\n",
    "    value_relations[relation].add(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate a global DataFrame with all the data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Generate a global DataFrame with all the data...\")\n",
    "# Convert the sets into a boolean lists\n",
    "value_relations = {relation: [entity in value_relations[relation]\n",
    "                              for entity in entities_df[\"Entity\"]] for relation in distinct_relations}\n",
    "# Convert to a Pandas Dataframe\n",
    "relations_df = pd.DataFrame(value_relations)\n",
    "global_df = pd.concat((entities_df, relations_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataframe city to continent\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataframe city to continent\")\n",
    "# Load the Dataframe of city to continent\n",
    "city_continent_df = pd.read_csv(\"data/city_global_location.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range ofr GridSearch\n",
    "# Define the hyperparameter grid\n",
    "param_grid = [\n",
    "  {'C': [0.1, 1, 2.5, 5, 7.5, 10], 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(ref, neighbours, df_with_labels, mode: str = \"one\"):\n",
    "    # Determine the label of the reference\n",
    "    label_ref = df_with_labels[df_with_labels['Entity']== ref]['labels'].to_list()[0]\n",
    "    \n",
    "    # Determine labels for all neighbors at once\n",
    "    all_neighbours_labels = np.array([df_with_labels[df_with_labels['Entity'] == name]['labels'].to_list()[0] for name in neighbours])\n",
    "\n",
    "    # Calculate intersection scores\n",
    "\n",
    "    if mode == \"one\":\n",
    "        intersect_labels = np.logical_and(label_ref, all_neighbours_labels)\n",
    "        # Sum the intersect labels along axis 1 (columns) and count non-zero elements\n",
    "        non_zero = np.count_nonzero(intersect_labels, axis=1)\n",
    "\n",
    "        # Count number of rows with non-zero elements\n",
    "        score = np.count_nonzero(non_zero)\n",
    "    elif mode == \"all\":\n",
    "        all_each_line = np.all(all_neighbours_labels == label_ref, axis=1)\n",
    "        # Count number of rows with non-zero elements\n",
    "        score = np.count_nonzero(all_each_line)\n",
    "    else:\n",
    "        raise ValueError(\"This is not a valid mode.\")\n",
    "\n",
    "    return score / len(neighbours)\n",
    "\n",
    "def closest_neighbours(ref_entity, other_entities, df_with_labels, k, mode: str = \"euclidean\"):\n",
    "    if mode == \"euclidean\":\n",
    "        relative_ref = other_entities - ref_entity\n",
    "        dist_to_ref = np.linalg.norm(relative_ref, axis=1)\n",
    "        k_closest_entity = np.argsort(dist_to_ref)[1:(k+1)]\n",
    "        return df_with_labels['Entity'].iloc[k_closest_entity]\n",
    "    elif mode == \"cosine\":\n",
    "        norm_ref = np.linalg.norm(ref_entity)\n",
    "        ref_norm = ref_entity / norm_ref\n",
    "        norm_other = np.linalg.norm(other_entities, axis=1)\n",
    "        normalized_other = np.divide(other_entities, norm_other.reshape(-1,1)) # division elementwise\n",
    "        cosine_sim = normalized_other.dot(ref_norm.T).reshape(-1,)\n",
    "        # We take the k elemnt with the highest score\n",
    "        k_closest_values = np.argsort(cosine_sim)[-(k+1):-1]\n",
    "        return df_with_labels['Entity'].iloc[k_closest_values]\n",
    "    raise ValueError(\"This is not a valid mode of similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SVM on gender and place of birth (Person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with preprocess for gender entities and location of birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all the data corresponding to person type\n",
    "entity_type = \"person_100007846\"\n",
    "\n",
    "# Triples with the relation wasBornIn\n",
    "triples_birth = triples_df[(triples_df['relation'] == 'wasBornIn')]\n",
    "birth_head = set(triples_birth['head'].to_list())\n",
    "birth_name = entities_df[entities_df['Entity'].isin(birth_head) & entities_df[\"type\"].apply(lambda x: entity_type in x)]\n",
    "birth_index = np.array(birth_name.index, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index for gender\n",
    "male_head = set(triples_df[(triples_df['relation'] == 'hasGender') & (triples_df['tail'] == 'male')]['head'].to_list())\n",
    "male_index = np.array(entities_df[entities_df['Entity'].isin(male_head) & (entities_df[\"type\"].apply(lambda x: entity_type in x))].index, dtype=int)\n",
    "female_head = set(triples_df[(triples_df['relation'] == 'hasGender') & (triples_df['tail'] == 'female')]['head'].to_list())\n",
    "female_index = np.array(entities_df[entities_df['Entity'].isin(female_head) & (entities_df[\"type\"].apply(lambda x: entity_type in x))].index, dtype=int)\n",
    "\n",
    "gender_head = set(triples_df[(triples_df['relation'] == 'hasGender')]['head'].to_list())\n",
    "gender_index = np.array(entities_df[entities_df['Entity'].isin(gender_head) & entities_df[\"type\"].apply(lambda x: entity_type in x)].index, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection of both indexes\n",
    "intersect_index = np.intersect1d(birth_index, gender_index)\n",
    "\n",
    "# Update the previous indexes\n",
    "male_index = np.intersect1d(intersect_index, male_index)\n",
    "female_index = np.intersect1d(intersect_index, female_index)\n",
    "birth_index = np.intersect1d(intersect_index, birth_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt scale embeddings to only concerned data\n",
    "scale_data = deepcopy(all_arrays)\n",
    "scale =StandardScaler()\n",
    "scale_data[intersect_index] = scale.fit_transform(scale_data[intersect_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correponding embedding\n",
    "male_embeddings = scale_data[male_index,:]\n",
    "female_embeddings = scale_data[female_index,:]\n",
    "\n",
    "gender_embeddings = np.concatenate((male_embeddings, female_embeddings))\n",
    "\n",
    "# Generate the labels\n",
    "labels_gender = np.array([0]*len(male_embeddings) + [1]*len(female_embeddings))\n",
    "\n",
    "# We split the entities\n",
    "X_gender_train, X_gender_test, y_gender_train, y_gender_test  = train_test_split(gender_embeddings, labels_gender, test_size=0.2, random_state=42)\n",
    "\n",
    "def isInEurope(city:str) -> bool:\n",
    "    \"\"\"Retrun True if the city is in Europe, otherwise False\"\"\"\n",
    "    city_row = city_continent_df[city_continent_df['cities'] == city]\n",
    "    return any(city_row['continents'] == 'Europe')\n",
    "\n",
    "# We select only the embeddings with the place of birth\n",
    "embeddings_city = scale_data[birth_index,:]\n",
    "\n",
    "# We generate the labels\n",
    "labels_place = np.array([isInEurope(triples_birth[triples_birth['head'] == entity]['tail'].item()) for entity in tqdm(entities_df['Entity'][intersect_index], total=len(entities_df['Entity'][intersect_index]))])\n",
    "\n",
    "# We store the indexes of places verifying the condition \n",
    "condition_index = np.where(labels_place == True)[0]\n",
    "not_condition_index = np.where(labels_place == False)[0]\n",
    "\n",
    "# We split the entities\n",
    "X_place_train, X_place_test, y_place_train, y_place_test  = train_test_split(embeddings_city, labels_place, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics gender\n",
    "nb_male = len(male_embeddings)\n",
    "nb_female = len(female_embeddings)\n",
    "nb_person = nb_male + nb_female\n",
    "print(f\"The number of male entities is {nb_male}, which is {round(100*nb_male/nb_person,2)} %\")\n",
    "print(f\"The number of female entities is {nb_female}, which is {round(100*nb_female/nb_person,2)} %\")\n",
    "print(f\"The total number of persons is {nb_person}\")\n",
    "\n",
    "#Statistics place\n",
    "nb_c = len(labels_place[labels_place == True])\n",
    "nb_nc = len(labels_place[labels_place == False])\n",
    "nb_condition = nb_c + nb_nc\n",
    "print(f\"The number of European entities is {nb_c}, which is {round(100*nb_c/nb_condition,2)} %\")\n",
    "print(f\"The number of non European entities is {nb_nc}, which is {round(100*nb_nc/nb_condition,2)} %\")\n",
    "print(f\"The total number of birth place is {nb_condition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SVM with GridSearch Cross validation\n",
    "grid_gender = GridSearchCV(SVC(class_weight='balanced', gamma='scale'), param_grid, refit = True, verbose = 1, scoring = make_scorer(cohen_kappa_score), n_jobs=16) \n",
    "grid_gender.fit(X_gender_train, y_gender_train)\n",
    "\n",
    "# Get best estimator\n",
    "clf_gender = grid_gender.best_estimator_\n",
    "print(\"The training accuracy is: {:.2f} %\".format(clf_gender.score(X_gender_train, y_gender_train)*100))\n",
    "print(\"The testing accuracy is: {:.2f} %\".format(clf_gender.score(X_gender_test, y_gender_test)*100))\n",
    "print(\"The kappa score (testing set) is {:.2f}\".format(cohen_kappa_score(clf_gender.predict(X_gender_test), y_gender_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_gender.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SVM\n",
    "grid_c = GridSearchCV(SVC(class_weight='balanced', gamma='scale'), param_grid, refit = True, verbose = 1, scoring = make_scorer(cohen_kappa_score), n_jobs=16) \n",
    "grid_c.fit(X_place_train, y_place_train)\n",
    "\n",
    "# Get best estimator\n",
    "clf_c = grid_c.best_estimator_\n",
    "print(\"The training accuracy is: {:.2f} %\".format(clf_c.score(X_place_train, y_place_train)*100))\n",
    "print(\"The testing accuracy is: {:.2f} %\".format(clf_c.score(X_place_test, y_place_test)*100))\n",
    "print(\"The kappa score (testing set) is {:.2f}\".format(cohen_kappa_score(clf_c.predict(X_place_test), y_place_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_c.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection in the new dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project and add bias through decision function\n",
    "entity_proj = np.concatenate((clf_gender.decision_function(scale_data).reshape(-1,1), clf_c.decision_function(scale_data).reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all labels for each axis\n",
    "all_labels_gender = np.concatenate((labels_gender.reshape(-1,1), labels_place.reshape(-1,1)), axis=1)\n",
    "intersect_entities = entities_df.iloc[intersect_index]\n",
    "intersect_entities[\"labels\"] = all_labels_gender.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k parameter\n",
    "k=10\n",
    "\n",
    "# Global similarity score\n",
    "scores_raw = 0\n",
    "scores_proj = 0\n",
    "for idx in tqdm(intersect_index):\n",
    "    # Choose a ref\n",
    "    ref_name = intersect_entities['Entity'][idx]\n",
    "\n",
    "    # Before projection\n",
    "    closest_entity_raw = closest_neighbours(scale_data[idx,:], scale_data[intersect_index], intersect_entities, k)\n",
    "    scores_raw += similarity_score(ref_name, closest_entity_raw, intersect_entities)\n",
    "\n",
    "    # After projection\n",
    "    closest_entity = closest_neighbours(entity_proj[idx,:], entity_proj[intersect_index], intersect_entities, k)\n",
    "    scores_proj += similarity_score(ref_name, closest_entity, intersect_entities)\n",
    "\n",
    "scores_raw /= len(intersect_index)\n",
    "scores_proj /= len(intersect_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The similarity score before projection of the top{k} entity is: {round(scores_raw,3)}\")\n",
    "print(f\"The similarity score after projection of the top{k} entity is: {round(scores_proj,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SVM on the relation isLocatedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all the data converted to a continent\n",
    "entity_type = \"city_108524735\"\n",
    "\n",
    "isLocated_df = pd.read_csv('data/isLocated_global.csv', index_col=0)\n",
    "all_unknown_location = set(isLocated_df[isLocated_df['continents'] == 'Unknown']['cities'].to_list())\n",
    "triples_location = triples_df[(triples_df['relation'] == 'isLocatedIn') & (~triples_df['head'].isin(all_unknown_location))]\n",
    "location_head = set(triples_location['head'].to_list())\n",
    "\n",
    "location_name = entities_df[entities_df['Entity'].isin(location_head) & entities_df[\"type\"].apply(lambda x: entity_type in x)]\n",
    "location_index = np.array(location_name.index, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale embeddings only to concerned data\n",
    "scale_location_data = deepcopy(all_arrays)\n",
    "scale = StandardScaler()\n",
    "scale_location_data[location_index] = scale.fit_transform(scale_location_data[location_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isinEuropeOrAsia(location) -> bool:\n",
    "    continent = isLocated_df[isLocated_df['cities'] == location]['continents'].item()\n",
    "    return continent == 'Europe' or continent == 'Asia'\n",
    "\n",
    "def isinEuropeOrNorthAmerica(location) -> bool:\n",
    "    continent = isLocated_df[isLocated_df['cities'] == location]['continents'].item()\n",
    "    return continent == 'Europe' or continent == 'North America'\n",
    "\n",
    "def isinNorthOrSouthAmerica(location) -> bool:\n",
    "    continent = isLocated_df[isLocated_df['cities'] == location]['continents'].item()\n",
    "    return continent == 'North America' or continent == 'South America'\n",
    "\n",
    "# Label entities according to the continent\n",
    "labels_EuAsia = np.array([isinEuropeOrAsia(loc) for loc in entities_df['Entity'][location_index]])\n",
    "X_EuAsia_train, X_EuAsia_test, y_EuAsia_train, y_EuAsia_test  = train_test_split(scale_location_data[location_index], labels_EuAsia, test_size=0.2, random_state=42)\n",
    "\n",
    "# Label entities according to the continent\n",
    "labels_EuNAme = np.array([isinEuropeOrNorthAmerica(loc) for loc in entities_df['Entity'][location_index]])\n",
    "X_EuNAme_train, X_EuNAme_test, y_EuNAme_train, y_EuNAme_test  = train_test_split(scale_location_data[location_index], labels_EuNAme, test_size=0.2, random_state=42)\n",
    "\n",
    "# Label entities according to the continent\n",
    "labels_NSAme = np.array([isinNorthOrSouthAmerica(loc) for loc in entities_df['Entity'][location_index]])\n",
    "X_NSAme_train, X_NSAme_test, y_NSAme_train, y_NSAme_test  = train_test_split(scale_location_data[location_index], labels_NSAme, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistics Europe or Asia\n",
    "nb_EuAsia = len(labels_EuAsia[labels_EuAsia == True])\n",
    "nb_total_loc = len(labels_EuAsia)\n",
    "print(f\"The number of entities in Europe or Asia is {nb_EuAsia}, which is {round(100*nb_EuAsia/nb_total_loc,2)} %\")\n",
    "\n",
    "#Statistics Europe or North America\n",
    "nb_EuNAme = len(labels_EuNAme[labels_EuNAme == True])\n",
    "print(f\"The number of elements in Europe or North America is {nb_EuNAme}, which is {round(100*nb_EuNAme/nb_total_loc,2)} %\")\n",
    "\n",
    "#Statistics North America or South America\n",
    "nb_NSAme = len(labels_NSAme[labels_NSAme == True])\n",
    "print(f\"The number of elements in Europe or North America is {nb_NSAme}, which is {round(100*nb_NSAme/nb_total_loc,2)} %\")\n",
    "\n",
    "\n",
    "print(f\"The total number of concerned places is {nb_total_loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SVM\n",
    "grid_EuAsia = GridSearchCV(SVC(class_weight='balanced', gamma='scale'), param_grid, refit = True, verbose = 1, scoring = make_scorer(cohen_kappa_score), n_jobs=16)\n",
    "grid_EuAsia.fit(X_EuAsia_train, y_EuAsia_train)\n",
    "# Get best estimator\n",
    "clf_EuAsia: SVC = grid_EuAsia.best_estimator_\n",
    "\n",
    "print(\"The training accuracy is: {:.2f} %\".format(clf_EuAsia.score(X_EuAsia_train, y_EuAsia_train)*100))\n",
    "print(\"The testing accuracy is: {:.2f} %\".format(clf_EuAsia.score(X_EuAsia_test, y_EuAsia_test)*100))\n",
    "print(\"The kappa score (testing set) is {:.2f}\".format(cohen_kappa_score(clf_EuAsia.predict(X_EuAsia_test), y_EuAsia_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_EuAsia.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SVM\n",
    "\n",
    "grid_EuNAme = GridSearchCV(SVC(class_weight='balanced', gamma='scale'), param_grid, refit = True, verbose = 1, scoring = make_scorer(cohen_kappa_score), n_jobs=16) \n",
    "grid_EuNAme.fit(X_EuNAme_train, y_EuNAme_train)\n",
    "# Get best estimator\n",
    "clf_EuNAme: SVC = grid_EuNAme.best_estimator_\n",
    "\n",
    "print(\"The training accuracy is: {:.2f} %\".format(clf_EuNAme.score(X_EuNAme_train, y_EuNAme_train)*100))\n",
    "print(\"The testing accuracy is: {:.2f} %\".format(clf_EuNAme.score(X_EuNAme_test, y_EuNAme_test)*100))\n",
    "print(\"The kappa score (testing set) is {:.2f}\".format(cohen_kappa_score(clf_EuNAme.predict(X_EuNAme_test), y_EuNAme_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_EuNAme.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process SVM\n",
    "grid_NSAme = GridSearchCV(SVC(class_weight='balanced', gamma='scale'), param_grid, refit = True, verbose = 1, scoring = make_scorer(cohen_kappa_score), n_jobs=16) \n",
    "grid_NSAme.fit(X_NSAme_train, y_NSAme_train)\n",
    "\n",
    "# Get best estimator\n",
    "clf_NSAme: SVC = grid_NSAme.best_estimator_\n",
    "print(\"The training accuracy is: {:.2f} %\".format(clf_NSAme.score(X_NSAme_train, y_NSAme_train)*100))\n",
    "print(\"The testing accuracy is: {:.2f} %\".format(clf_NSAme.score(X_NSAme_test, y_NSAme_test)*100))\n",
    "print(\"The kappa score (testing set) is {:.2f}\".format(cohen_kappa_score(clf_NSAme.predict(X_NSAme_test), y_NSAme_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_NSAme.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project and add bias through decision function\n",
    "entity_continent_proj = np.concatenate((clf_EuAsia.decision_function(scale_location_data).reshape(-1,1), clf_EuNAme.decision_function(scale_location_data).reshape(-1,1), clf_NSAme.decision_function(scale_location_data).reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_asia_index = np.where(labels_EuAsia == True)[0]\n",
    "europe_namerica_index = np.where(labels_EuNAme == True)[0]\n",
    "north_souh_america_index = np.where(labels_NSAme == True)[0]\n",
    "not_north_souh_america_index = np.where(labels_NSAme == False)[0]\n",
    "not_europe_asia_index = np.where(labels_EuAsia == False)[0]\n",
    "not_europe_namerica_index = np.where(labels_EuNAme == False)[0]\n",
    "\n",
    "europe_index = np.intersect1d(europe_asia_index, europe_namerica_index)\n",
    "asia_index = np.intersect1d(europe_asia_index, not_europe_namerica_index)\n",
    "namerica_index = np.intersect1d(europe_namerica_index, not_europe_asia_index)\n",
    "samerica_index = np.intersect1d(north_souh_america_index, not_europe_namerica_index)\n",
    "\n",
    "other_index = np.intersect1d(np.intersect1d(not_europe_asia_index, not_europe_namerica_index), not_north_souh_america_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all labels for each axis\n",
    "all_labels_location = np.concatenate((labels_EuAsia.reshape(-1,1), labels_EuNAme.reshape(-1,1), labels_NSAme.reshape(-1,1)), axis=1)\n",
    "location_name[\"labels\"] = all_labels_location.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_labels_location[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k parameter\n",
    "k=10\n",
    "\n",
    "# Global similarity score\n",
    "scores_raw = 0\n",
    "scores_proj = 0\n",
    "for idx in tqdm(location_index):\n",
    "    # Choose a ref\n",
    "    ref_proj = entity_continent_proj[idx,:]\n",
    "    ref_name = location_name['Entity'][idx]\n",
    "\n",
    "    # Before projection\n",
    "    closest_entity_raw = closest_neighbours(scale_location_data[idx,:], scale_location_data[location_index], location_name, k, mode='cosine')\n",
    "    scores_raw += similarity_score(ref_name, closest_entity_raw, location_name, mode=\"all\")\n",
    "\n",
    "    # After projection\n",
    "    closest_entity = closest_neighbours(ref_proj, entity_continent_proj[location_index], location_name, k, mode='cosine')\n",
    "    scores_proj += similarity_score(ref_name, closest_entity, location_name, mode=\"all\")\n",
    "\n",
    "scores_raw /= len(location_index)\n",
    "scores_proj /= len(location_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The similarity score before projection of the top{k} entity is: {round(scores_raw,3)}\")\n",
    "print(f\"The similarity score after projection of the top{k} entity is: {round(scores_proj,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM on Scientists awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all the data converted to a scientist\n",
    "entity_type = \"scientist_110560637\"\n",
    "\n",
    "triples_award = triples_df[(triples_df['relation'] == 'hasWonPrize')]\n",
    "award_head = set(triples_award['head'].to_list())\n",
    "scientist_award_head = entities_df[entities_df['Entity'].isin(award_head) & entities_df[\"type\"].apply(lambda x: entity_type in x)]\n",
    "scientist_award_index = np.array(scientist_award_head.index, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale embeddings only to concerned data\n",
    "scale_scientist_data = deepcopy(all_arrays)\n",
    "scale = StandardScaler()\n",
    "scale_scientist_data[scientist_award_index] = scale.fit_transform(scale_scientist_data[scientist_award_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_awards = [\"National_Medal_of_Science\", \"Nobel_Prize_in_Physics\", \"Copley_Medal\", \"Royal_Medal\", \"Nobel_Prize_in_Physiology_or_Medicine\", \"Nobel_Prize_in_Chemistry\"]\n",
    "SVC_awards: list[GridSearchCV] = [GridSearchCV(SVC(class_weight='balanced'), param_grid, refit = True, verbose = 1, scoring = make_scorer(cohen_kappa_score), n_jobs=16)]*len(top_awards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_award = [np.array([award in set(triples_award[triples_award['head'] == scientist]['tail'].to_list()) for scientist in entities_df['Entity'][scientist_award_index]]) for award in top_awards]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The training accuracy is: 100.00 %\n",
      "The testing accuracy is: 100.00 %\n",
      "The testing kappa score is 1.00\n",
      "{'C': 2.5, 'kernel': 'rbf'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The training accuracy is: 100.00 %\n",
      "The testing accuracy is: 99.62 %\n",
      "The testing kappa score is 0.98\n",
      "{'C': 1, 'kernel': 'rbf'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The training accuracy is: 100.00 %\n",
      "The testing accuracy is: 99.25 %\n",
      "The testing kappa score is 0.95\n",
      "{'C': 2.5, 'kernel': 'rbf'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The training accuracy is: 100.00 %\n",
      "The testing accuracy is: 100.00 %\n",
      "The testing kappa score is 1.00\n",
      "{'C': 2.5, 'kernel': 'rbf'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The training accuracy is: 100.00 %\n",
      "The testing accuracy is: 99.25 %\n",
      "The testing kappa score is 0.94\n",
      "{'C': 1, 'kernel': 'rbf'}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "The training accuracy is: 100.00 %\n",
      "The testing accuracy is: 100.00 %\n",
      "The testing kappa score is 1.00\n",
      "{'C': 2.5, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "decision_scientist = []\n",
    "for idx, svm in enumerate(SVC_awards):\n",
    "    # We split the entities\n",
    "    X_award_train, X_award_test, y_award_train, y_award_test  = train_test_split(scale_scientist_data[scientist_award_index], labels_award[idx], test_size=0.2, random_state=42)\n",
    "    svm.fit(X_award_train, y_award_train)\n",
    "    print(\"The training accuracy is: {:.2f} %\".format(svm.best_estimator_.score(X_award_train, y_award_train)*100))\n",
    "    print(\"The testing accuracy is: {:.2f} %\".format(svm.best_estimator_.score(X_award_test, y_award_test)*100))\n",
    "    print(\"The testing kappa score is {:.2f}\".format(cohen_kappa_score(svm.best_estimator_.predict(X_award_test), y_award_test)))\n",
    "    print(svm.best_params_)\n",
    "    decision_scientist.append(svm.best_estimator_.decision_function(scale_scientist_data).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project into the 2d space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project and add bias through decision function\n",
    "scientist_award_proj = np.concatenate(decision_scientist, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_174761/223026840.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  scientist_award_head[\"labels\"] = labels_award.tolist()\n"
     ]
    }
   ],
   "source": [
    "# Add a new column with all labels\n",
    "labels_award = np.array(labels_award).T\n",
    "scientist_award_head[\"labels\"] = labels_award.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1323/1323 [00:18<00:00, 73.04it/s] \n"
     ]
    }
   ],
   "source": [
    "# Global similarity score\n",
    "k=10\n",
    "scores_raw = 0\n",
    "scores_proj = 0\n",
    "for idx in tqdm(scientist_award_index):\n",
    "    # Choose a ref\n",
    "    ref_proj = scientist_award_proj[idx,:]\n",
    "    ref_name = scientist_award_head['Entity'][idx]\n",
    "\n",
    "    # Before projection\n",
    "    ref_entity = scale_scientist_data[idx,:]\n",
    "    closest_entity_raw = closest_neighbours(ref_entity, scale_scientist_data[scientist_award_index], scientist_award_head, k, mode='cosine')\n",
    "    scores_raw += similarity_score(ref_name, closest_entity_raw, scientist_award_head, mode='all')\n",
    "\n",
    "    # After projection\n",
    "    closest_entity = closest_neighbours(ref_proj, scientist_award_proj[scientist_award_index], scientist_award_head, k, mode='cosine')\n",
    "    scores_proj += similarity_score(ref_name, closest_entity, scientist_award_head, mode='all')\n",
    "\n",
    "scores_raw /= len(scientist_award_index)\n",
    "scores_proj /= len(scientist_award_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity score before projection of the top10 entity is: 0.578\n",
      "The similarity score after projection of the top10 entity is: 0.972\n"
     ]
    }
   ],
   "source": [
    "print(f\"The similarity score before projection of the top{k} entity is: {round(scores_raw,3)}\")\n",
    "print(f\"The similarity score after projection of the top{k} entity is: {round(scores_proj,3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
